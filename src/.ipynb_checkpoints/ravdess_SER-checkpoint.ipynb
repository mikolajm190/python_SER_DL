{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5553b0f1",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d94e55",
   "metadata": {},
   "source": [
    "## emotion label + path to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7ea6f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/ravdess/Audio_Speech_Actors_01-24/Acto...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/ravdess/Audio_Speech_Actors_01-24/Acto...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/ravdess/Audio_Speech_Actors_01-24/Acto...</td>\n",
       "      <td>fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/ravdess/Audio_Speech_Actors_01-24/Acto...</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/ravdess/Audio_Speech_Actors_01-24/Acto...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2447</th>\n",
       "      <td>../data/ravdess/Audio_Song_Actors_01-24/Actor_...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>../data/ravdess/Audio_Song_Actors_01-24/Actor_...</td>\n",
       "      <td>fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2449</th>\n",
       "      <td>../data/ravdess/Audio_Song_Actors_01-24/Actor_...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2450</th>\n",
       "      <td>../data/ravdess/Audio_Song_Actors_01-24/Actor_...</td>\n",
       "      <td>fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2451</th>\n",
       "      <td>../data/ravdess/Audio_Song_Actors_01-24/Actor_...</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2452 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   path  emotion\n",
       "0     ../data/ravdess/Audio_Speech_Actors_01-24/Acto...    angry\n",
       "1     ../data/ravdess/Audio_Speech_Actors_01-24/Acto...    angry\n",
       "2     ../data/ravdess/Audio_Speech_Actors_01-24/Acto...  fearful\n",
       "3     ../data/ravdess/Audio_Speech_Actors_01-24/Acto...     calm\n",
       "4     ../data/ravdess/Audio_Speech_Actors_01-24/Acto...    happy\n",
       "...                                                 ...      ...\n",
       "2447  ../data/ravdess/Audio_Song_Actors_01-24/Actor_...    happy\n",
       "2448  ../data/ravdess/Audio_Song_Actors_01-24/Actor_...  fearful\n",
       "2449  ../data/ravdess/Audio_Song_Actors_01-24/Actor_...    angry\n",
       "2450  ../data/ravdess/Audio_Song_Actors_01-24/Actor_...  fearful\n",
       "2451  ../data/ravdess/Audio_Song_Actors_01-24/Actor_...      sad\n",
       "\n",
       "[2452 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/home/judehey/miniconda3/envs/myenv/bin/python\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "pathToData=\"../data/ravdess/\"\n",
    "imgsPath=\"../imgs/ravdess/\"\n",
    "modelsPath=\"../models/ravdess/\"\n",
    "\n",
    "def load_data(path):\n",
    "    emotion, file_path = [], []\n",
    "    \n",
    "    # iterate over song, speech folders, every actor subfolder and audio files in\n",
    "    for folder in os.listdir(path):\n",
    "        for subfolder in os.listdir(path + folder):\n",
    "            for file in os.listdir(path + folder + \"/\" + subfolder):\n",
    "                emotion.append(file.split(\"-\")[2])\n",
    "                file_path.append(path + folder + \"/\" + subfolder + \"/\" + file)\n",
    "                \n",
    "    # prepare dataframe\n",
    "    audio_df = pd.DataFrame(emotion)\n",
    "    audio_df = audio_df.replace({\"01\": \"neutral\", \"02\": \"calm\", \"03\": \"happy\", \"04\": \"sad\", \"05\": \"angry\", \"06\": \"fearful\", \"07\": \"disgust\", \"08\": \"surprised\"})\n",
    "    \n",
    "    # concatenate file paths to emotion data frame\n",
    "    audio_df = pd.concat([pd.DataFrame(file_path), audio_df], axis=1)\n",
    "    audio_df.columns = ['path', 'emotion']\n",
    "    \n",
    "    return audio_df\n",
    "\n",
    "df = load_data(pathToData)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7d8c62",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "\n",
    "- [mfcc extraction info](https://www.researchgate.net/profile/Teddy-Gunawan/publication/353296706_Speech_Emotion_Recognition_Using_Feature_Fusion_of_TEO_and_MFCC_on_Multilingual_Databases/links/613ef96f4e1df2710631ca0a/Speech-Emotion-Recognition-Using-Feature-Fusion-of-TEO-and-MFCC-on-Multilingual-Databases.pdf)\n",
    "- [librosa docs](https://librosa.org/doc/latest/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1d15e283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>emotion</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>335</th>\n",
       "      <th>336</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/ravdess/Audio_Speech_Actors_01-24/Acto...</td>\n",
       "      <td>angry</td>\n",
       "      <td>-56.247910</td>\n",
       "      <td>-60.189079</td>\n",
       "      <td>-62.431377</td>\n",
       "      <td>-65.687004</td>\n",
       "      <td>-65.071297</td>\n",
       "      <td>-61.132690</td>\n",
       "      <td>-59.303288</td>\n",
       "      <td>-58.952614</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/ravdess/Audio_Speech_Actors_01-24/Acto...</td>\n",
       "      <td>angry</td>\n",
       "      <td>-53.630333</td>\n",
       "      <td>-57.621811</td>\n",
       "      <td>-59.392071</td>\n",
       "      <td>-64.770775</td>\n",
       "      <td>-66.467354</td>\n",
       "      <td>-65.367233</td>\n",
       "      <td>-63.528645</td>\n",
       "      <td>-62.816776</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/ravdess/Audio_Speech_Actors_01-24/Acto...</td>\n",
       "      <td>fearful</td>\n",
       "      <td>-46.282841</td>\n",
       "      <td>-51.266727</td>\n",
       "      <td>-56.261574</td>\n",
       "      <td>-61.549080</td>\n",
       "      <td>-60.599564</td>\n",
       "      <td>-59.163658</td>\n",
       "      <td>-57.306171</td>\n",
       "      <td>-55.181694</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/ravdess/Audio_Speech_Actors_01-24/Acto...</td>\n",
       "      <td>calm</td>\n",
       "      <td>-42.580002</td>\n",
       "      <td>-54.126614</td>\n",
       "      <td>-58.591011</td>\n",
       "      <td>-63.371849</td>\n",
       "      <td>-62.504086</td>\n",
       "      <td>-63.478996</td>\n",
       "      <td>-68.120491</td>\n",
       "      <td>-70.887253</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/ravdess/Audio_Speech_Actors_01-24/Acto...</td>\n",
       "      <td>happy</td>\n",
       "      <td>-43.760826</td>\n",
       "      <td>-52.314262</td>\n",
       "      <td>-60.720554</td>\n",
       "      <td>-67.258759</td>\n",
       "      <td>-63.788132</td>\n",
       "      <td>-63.236649</td>\n",
       "      <td>-65.145950</td>\n",
       "      <td>-70.562210</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 475 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  emotion          0  \\\n",
       "0  ../data/ravdess/Audio_Speech_Actors_01-24/Acto...    angry -56.247910   \n",
       "1  ../data/ravdess/Audio_Speech_Actors_01-24/Acto...    angry -53.630333   \n",
       "2  ../data/ravdess/Audio_Speech_Actors_01-24/Acto...  fearful -46.282841   \n",
       "3  ../data/ravdess/Audio_Speech_Actors_01-24/Acto...     calm -42.580002   \n",
       "4  ../data/ravdess/Audio_Speech_Actors_01-24/Acto...    happy -43.760826   \n",
       "\n",
       "           1          2          3          4          5          6  \\\n",
       "0 -60.189079 -62.431377 -65.687004 -65.071297 -61.132690 -59.303288   \n",
       "1 -57.621811 -59.392071 -64.770775 -66.467354 -65.367233 -63.528645   \n",
       "2 -51.266727 -56.261574 -61.549080 -60.599564 -59.163658 -57.306171   \n",
       "3 -54.126614 -58.591011 -63.371849 -62.504086 -63.478996 -68.120491   \n",
       "4 -52.314262 -60.720554 -67.258759 -63.788132 -63.236649 -65.145950   \n",
       "\n",
       "           7  ...  335  336  337  338  339  340  341  342  343  344  \n",
       "0 -58.952614  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1 -62.816776  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "2 -55.181694  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "3 -70.887253  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "4 -70.562210  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 475 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "# log-mel spectrogram into mfcc\n",
    "\n",
    "logMelSpecVal = []\n",
    "\n",
    "def parse_audio_file(pathToFile, _duration, sampleRate, _offset):\n",
    "    # load file\n",
    "    wavf, sample_rate = librosa.load(pathToFile, res_type=\"kaiser_fast\", duration=_duration, sr=sampleRate, offset=_offset)\n",
    "    \n",
    "    # get log-mel spectrogram\n",
    "    spectrogram = librosa.feature.melspectrogram(y=wavf, sr=sample_rate, n_mels=128, fmax=8000)\n",
    "    \n",
    "    # get mfcc features (coefficients = 13, filters = 32)\n",
    "    mfcc = np.array([])\n",
    "    mfcc = np.mean(librosa.feature.mfcc(S=librosa.power_to_db(spectrogram), n_mfcc = 13), axis=0)\n",
    "    \n",
    "    return mfcc\n",
    "\n",
    "mfccVal = []\n",
    "\n",
    "for fpath in df['path']:\n",
    "    mfcc = parse_audio_file(fpath, 4, 44100, 0.5)\n",
    "    mfccVal.append(mfcc)\n",
    "    \n",
    "# drop unnecessary column\n",
    "df.drop(columns='path', inplace=True)\n",
    "df = df.fillna(0)\n",
    "\n",
    "# fill na's\n",
    "    \n",
    "df = pd.concat([df, pd.DataFrame(mfccVal)], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3512e078",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d077003",
   "metadata": {},
   "source": [
    "## class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "901912f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['angry' 'fearful' 'calm' 'happy' 'sad' 'surprised' 'disgust' 'neutral']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# class imbalance plot\n",
    "def class_imbalance_plot():\n",
    "    print(df['emotion'].unique())\n",
    "    sns.set_theme(style='darkgrid')\n",
    "    sns.countplot(x = 'emotion', data = df)\n",
    "    plt.title('Class distribution')\n",
    "    plt.savefig(imgsPath + 'class_imbalance.png')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "class_imbalance_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115ac338",
   "metadata": {},
   "source": [
    "## waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbc0634",
   "metadata": {},
   "source": [
    "## log-mel spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69012de",
   "metadata": {},
   "source": [
    "# Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cc0752c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0          1          2          3          4          5    \\\n",
      "735  -79.276497 -76.247894 -74.681320 -74.286652 -73.341515 -72.162033   \n",
      "345  -75.829987 -75.919518 -75.707474 -74.842819 -75.187515 -75.072441   \n",
      "141  -54.807732 -55.681580 -59.342587 -63.000244 -64.967361 -68.056229   \n",
      "132  -47.265152 -54.695454 -62.173210 -65.413322 -63.428444 -61.643948   \n",
      "1028 -90.918671 -89.427055 -88.676552 -87.601204 -85.492477 -83.549286   \n",
      "\n",
      "            6          7          8          9    ...  335  336  337  338  \\\n",
      "735  -70.286987 -71.272018 -71.299538 -70.486069  ...  0.0  0.0  0.0  0.0   \n",
      "345  -75.994179 -75.687630 -75.126045 -74.819473  ...  0.0  0.0  0.0  0.0   \n",
      "141  -67.891434 -69.526611 -72.907043 -72.454704  ...  0.0  0.0  0.0  0.0   \n",
      "132  -62.301262 -62.971416 -63.544735 -62.948200  ...  0.0  0.0  0.0  0.0   \n",
      "1028 -81.687187 -80.009239 -80.560951 -82.360855  ...  0.0  0.0  0.0  0.0   \n",
      "\n",
      "      339  340  341  342  343  344  \n",
      "735   0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "345   0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "141   0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "132   0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1028  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 473 columns]\n",
      "735       disgust\n",
      "345     surprised\n",
      "141         angry\n",
      "132           sad\n",
      "1028         calm\n",
      "Name: emotion, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def data_split(df):\n",
    "    # divide data into labels and features\n",
    "    X = df.iloc[:, :].drop(columns=['emotion'])\n",
    "    y = df['emotion']\n",
    "    \n",
    "    # stratified train, test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=0, stratify=y)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = data_split(df)\n",
    "\n",
    "print(X_train.head())\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb2f61e",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9eff4582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1961,)\n",
      "(491,)\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "['angry' 'calm' 'disgust' 'fearful' 'happy' 'neutral' 'sad' 'surprised']\n",
      "(1961, 473, 1)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "def data_preprocessing(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    # normalization (z-score, values between -1 and 1)\n",
    "    mean = np.mean(X_train, axis=0)\n",
    "    std = np.std(X_train, axis=0)\n",
    "    X_train = (X_train - mean)/std\n",
    "    X_test = (X_test - mean)/std\n",
    "\n",
    "    # turn data into arrays for keras\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "\n",
    "    # label one hot encoding\n",
    "    lb = LabelEncoder()\n",
    "    y_train = to_categorical(lb.fit_transform(y_train))\n",
    "    y_test = to_categorical(lb.fit_transform(y_test))\n",
    "\n",
    "    print(y_test[0:3])\n",
    "\n",
    "    print(lb.classes_)\n",
    "\n",
    "    # data reshaping\n",
    "    X_train = X_train[:,:,np.newaxis]\n",
    "    X_test = X_test[:,:,np.newaxis]\n",
    "\n",
    "    print(X_train.shape)\n",
    "    \n",
    "    return\n",
    "\n",
    "data_preprocessing(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9081fbd",
   "metadata": {},
   "source": [
    "# Model architecture\n",
    "\n",
    "- [model architecture info](https://www.researchgate.net/profile/Gregor-Hofer-3/publication/335829168_Analysis_of_Deep_Learning_Architectures_for_Cross-Corpus_Speech_Emotion_Recognition/links/5e2f0a2f4585152d156d9f4f/Analysis-of-Deep-Learning-Architectures-for-Cross-Corpus-Speech-Emotion-Recognition.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a1322ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 459, 96)           1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 459, 96)           384       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 229, 96)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 225, 256)          123136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 225, 256)          1024      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 112, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 110, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 110, 256)          1024      \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 108, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 108, 256)          1024      \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 106, 128)          98432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 106, 128)          512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 52, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 50, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 50, 128)           512       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50, 1024)          132096    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50, 1024)          1049600   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 50, 8)             8200      \n",
      "=================================================================\n",
      "Total params: 1,860,488\n",
      "Trainable params: 1,858,248\n",
      "Non-trainable params: 2,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "def get_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv1D(96, kernel_size=(15), activation=\"relu\", input_shape=(X_train.shape[1],1)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(pool_size=(3), strides=(2)))\n",
    "    \n",
    "    model.add(layers.Conv1D(256, kernel_size=(5), activation=\"relu\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(pool_size=(3), strides=(2)))\n",
    "    \n",
    "    model.add(layers.Conv1D(256, kernel_size=(3), activation=\"relu\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.Conv1D(256, kernel_size=(3), activation=\"relu\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.Conv1D(128, kernel_size=(3), activation=\"relu\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(pool_size=(3), strides=(2)))\n",
    "    \n",
    "    model.add(layers.Conv1D(128, kernel_size=(3), activation=\"relu\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.Dense(1024, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1024, activation=\"relu\"))\n",
    "    model.add(layers.Dense(8, activation=\"relu\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45c0b86",
   "metadata": {},
   "source": [
    "# Model training and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "88cbee52",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/judehey/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /home/judehey/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/judehey/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/judehey/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/judehey/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/judehey/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /home/judehey/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:754 train_step\n        y_pred = self(x, training=True)\n    /home/judehey/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /home/judehey/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/keras/engine/input_spec.py:234 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential_11 is incompatible with the layer: : expected min_ndim=3, found ndim=2. Full shape received: (None, 473)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [105]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(modelsPath \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_1_CNN\u001b[39m\u001b[38;5;124m'\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# fit model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model_history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# plot model accuracy and loss over epochs\u001b[39;00m\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(model_history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1100\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1095\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1096\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1097\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1098\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1099\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1100\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1102\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name) \u001b[38;5;28;01mas\u001b[39;00m tm:\n\u001b[0;32m--> 828\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    829\u001b[0m   compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m   new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:871\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m   initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 871\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[1;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:725\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lifted_initializer_graph \u001b[38;5;241m=\u001b[39m lifted_initializer_graph\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_deleter \u001b[38;5;241m=\u001b[39m FunctionDeleter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lifted_initializer_graph)\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_stateful_fn \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 725\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_internal_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    726\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[1;32m    729\u001b[0m   \u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2969\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2968\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m-> 2969\u001b[0m   graph_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2970\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3361\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3357\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_define_function_with_shape_relaxation(\n\u001b[1;32m   3358\u001b[0m       args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[1;32m   3360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mmissed\u001b[38;5;241m.\u001b[39madd(call_context_key)\n\u001b[0;32m-> 3361\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3362\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mprimary[cache_key] \u001b[38;5;241m=\u001b[39m graph_function\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3196\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3191\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3192\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m   3193\u001b[0m ]\n\u001b[1;32m   3194\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[1;32m   3195\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[0;32m-> 3196\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3199\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3204\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3205\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   3207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[1;32m   3208\u001b[0m     function_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[1;32m   3209\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m   3210\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m   3211\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m   3212\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   3214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:990\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    988\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m--> 990\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m    994\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(convert, func_outputs,\n\u001b[1;32m    995\u001b[0m                                   expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:634\u001b[0m, in \u001b[0;36mFunction._defun_with_scope.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m     xla_context\u001b[38;5;241m.\u001b[39mExit()\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 634\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:977\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    976\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 977\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m    978\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/judehey/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /home/judehey/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/judehey/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/judehey/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/judehey/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/judehey/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /home/judehey/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:754 train_step\n        y_pred = self(x, training=True)\n    /home/judehey/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /home/judehey/miniconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/keras/engine/input_spec.py:234 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential_11 is incompatible with the layer: : expected min_ndim=3, found ndim=2. Full shape received: (None, 473)\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# use checkpoints to save best model\n",
    "checkpoint = ModelCheckpoint(modelsPath + 'model_1_CNN', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# fit model\n",
    "model_history = model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test), callbacks=[checkpoint], verbose=0)\n",
    "\n",
    "# plot model accuracy and loss over epochs\n",
    "plt.plot(model_history.history['accuracy'])\n",
    "plt.plot(model_history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(imgsPath + 'Initial_Model_Accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(imgsPath + 'Initial_Model_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0fe13d",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4736eca",
   "metadata": {},
   "source": [
    "## accuracy and loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd66f9b",
   "metadata": {},
   "source": [
    "## confusion matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
